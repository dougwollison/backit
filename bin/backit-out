#!/usr/bin/env python3
import os
import sys
import gc
import tarfile
import shutil
from glob import glob

import shared as backit
from shared import option as date
from job import JobFile

tarballs_dir = os.path.realpath( backit.get( 'storage', 'tarballs_dir' ) )
backups_root = os.path.realpath( backit.get( 'rsync', 'backups_root' ) )
part_size = int( backit.get( 'backblaze', 'part_size' ) )
account_id = backit.get( 'backblaze', 'account_id' )
account_key = backit.get( 'backblaze', 'account_key' )
bucket = backit.get( 'backblaze', 'bucket' )

parent_folder = backit.get( 'backblaze', 'parent_folder', '' )
separate_folders = backit.get( 'backblaze', 'separate_folders', False )

# Get the specified or otherwise latest backup
if date :
	print( backups_root, date )
	backup_dir = backit.pathit( backups_root, date )
else :
	backup_file = sorted( glob( backups_root + '/20*_*.json' ), reverse=True )[0]
	backup_dir = backup_file.replace( '.json', '' )

backup = JobFile( backup_dir )

# Ensure the archive exists
if not os.path.isdir( backup_dir ) :
	print( 'Archive not found: ' + backup_dir )
	sys.exit()

# Check if the backup_dir is somehow still downloading
if backup.isStatus( 'downloading' ) :
	print( 'Unable to archive ' + backup_dir + ' (download in progress).' )
	sys.exit()

print( 'Exporting %s to B2...' % backup_dir )

# (Re)Authorize for B2
os.system( 'b2 authorize-account %s %s' % ( account_id, account_key ) )

def mkdir( dir ) :
	"""Shorthand for creating a directory that doesn't exist yet"""

	if not os.path.exists( dir ) :
		os.makedirs( dir )

	return dir

mkdir( tarballs_dir )

def make_archive( folder, parent = '', prefix = '' ) :
	"""Create a gzipped tarball of the folder and upload it to B2"""

	basename = prefix + os.path.basename( folder );
	b2_filename = backit.pathit( parent_folder, os.path.basename( parent ), basename + '.tgz' )
	tar_file = backit.pathit( tarballs_dir, b2_filename )
	tar_dir = os.path.dirname( tar_file )

	# If straight up done, skip
	if backup.isFile( b2_filename, 'done' ) :
		print( 'Skipping ' + folder )
		return

	mkdir( tar_dir )

	# (Re)create the archive if not yet ready
	if not backup.isFile( b2_filename, 'ready' ) :
		print( 'Archiving ' + folder + '...' )

		tar = tarfile.open( tar_file, 'w:gz' )
		tar.add( folder, arcname = basename )
		tar.close()

		# Paranoid cleanup
		del tar
		gc.collect()

		# Flag as ready
		backup.logFile( b2_filename, 'ready' )

		print( 'Done.' )

	print( 'Uploading ' + tar_file + '...' )

	# Upload to B2
	status = os.system( 'b2 upload-file %s %s %s' % ( bucket, tar_file, b2_filename ) )

	if not status == 0 :
		print( 'Upload error. Aborting.' )
		sys.exit()

	print( 'Done.' )

	print( 'Cleaning up...' )

	# Remove the tar file
	os.remove( tar_file )

	# Flag as done
	backup.logFile( b2_filename, 'done' )

	print( 'Done.' )

# Flag as being in the process of archiving
backup.setStatus( 'uploading' )

if separate_folders :
	folders = separate_folders.split( ':' )

	for folder in folders :
		folder = backit.pathit( backup_dir, folder )

		subfolders = sorted( glob( folder ) )

		if len( subfolders ) > 1 :
			prefix = os.path.basename( os.path.dirname( folder ) ) + '-'

			for dir in subfolders :
				make_archive( dir, backup_dir, prefix )
		else :
			make_archive( folder, backup_dir )

else :
	make_archive( backup_dir )

shutil.rmtree( tarballs_dir )

backup.setStatus( 'archived' )

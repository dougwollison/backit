#!/usr/bin/env python3
import time
import os
import subprocess
from glob import glob

import shared as backit
from shared import option as testing
from job import JobFile

filter = backit.get( 'rsync', 'filter' )
log_file = backit.get( 'rsync', 'log_file' )
connection = backit.get( 'rsync', 'connection' )
rsync_path = backit.get( 'rsync', 'executable', 'rsync' )
source_root = os.path.realpath( backit.get( 'rsync', 'source_root', '/' ) )
backups_root = os.path.realpath( backit.get( 'rsync', 'backups_root' ) )

timestamp = time.strftime( '%Y.%m.%d_%H.%M.%S' )
backup_dir = backit.pathit( backups_root, timestamp )
prevlink = backit.pathit( backups_root, 'latest' )

# Build the rsync command
command = [
	'/usr/bin/rsync',
	'-vhrtlH',
	'-e', 'ssh',
	'--rsync-path', rsync_path,
	'--log-file', log_file,
]

# If test flag is present, do a dry run
if testing :
	command.append( '--dry-run' )

# Add the filter file if it exists
if log_file and os.path.exists( log_file ) :
	command.extend( [ '--log-file', log_file ] )

# Add the filter file if it exists
if filter and os.path.exists( filter ) :
	command.extend( [ '--filter', '. ' + filter ] )

# If the /latest link exists, add flags for incremental backup
if os.path.exists( prevlink ) :
	command = command + [
		'--delete',
		'--delete-excluded',
		'--link-dest', prevlink
	]

# Add the source and destination
command.append( connection + ':' + source_root )
command.append( backup_dir )

# Create the backup_dir directory
if not testing :
	os.makedirs( backup_dir )

# Create the job for it
backup = JobFile( backup_dir, 'downloading' )

# Execute the rsync command
subprocess.call( command )

# Clean up
if not testing :
	# Remove the previous /latest link
	if os.path.exists( prevlink ) :
		os.unlink( prevlink )

	# Add the new /latest link, moving to the parent folder to ensure it's relative
	start = os.getcwd()
	os.chdir( backups_root )
	os.symlink( os.path.basename( backup_dir ), 'latest' )
	os.chdir( start )

# Remove the download flag
backup.setStatus( 'saved' )

#!/usr/bin/env python3
import sys
import os
import shutil
from glob import glob

import shared as backit
from shared import option as testing
from job import JobFile

testing = bool( testing )
keep = int( backit.get( 'storage', 'keep', 0 ) )

if keep < 1 :
	print( 'Aborting cleanup; number of backups requested to keep is less than 1.' )
	sys.exit()

backups_root = os.path.realpath( backit.get( 'rsync', 'backups_root' ) )

found_backups = sorted( glob( backups_root + '/20*_*.json' ) )
count = len( found_backups )

print( 'Found %d archive(s).' % count );

if count > keep :
	print( 'Need to delete earliest %d.' % ( count - keep ) )

	n = 0
	while count > keep :
		# Get the earliest backup
		backup_dir = found_backups[ n ].replace( '.json', '' )
		n += 1

		# Load it's project file
		backup = JobFile( backup_dir )

		# Check if it's incomplete
		if backup.isStatus( 'downloading' ):
			print( 'Unable to delete ' + backup_dir + ' (download in progress). Skipping.' )
			continue

		# Check if it's still uploading
		if backup.isStatus( 'uploading' ) :
			print( 'Unable to delete ' + backup_dir + ' (upload in progress). Skipping.' )
			continue

		# Recursively delete the archive, advance
		if testing :
			print( 'Will delete ' + backup_dir + '.' )
		else :
			print( 'Deleting ' + backup_dir + '...' )
			shutil.rmtree( backup_dir )
			os.remove( backup.file )
			print( 'Done.' )

		count -= 1
else:
	print( 'No cleanup necessary.' )
